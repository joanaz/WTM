{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to train a neural network with one hidden layer on MNIST. At the end is a short exercise to add a second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = None\n",
    "def ResetSession():\n",
    "    tf.reset_default_graph()\n",
    "    global sess\n",
    "    if sess is not None: sess.close()\n",
    "    sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for a single hidden layer nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ResetSession()\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_PIXELS = 28 * 28\n",
    "TRAIN_STEPS = 1000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "HIDDEN1_UNITS = 128\n",
    "LEARNING_RATE = 0.5\n",
    "\n",
    "# As before, here are our placeholders for images and labels\n",
    "x = tf.placeholder(tf.float32, [None, NUM_PIXELS], name=\"pixels\")\n",
    "y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES], name=\"labels\")\n",
    "\n",
    "def weight_variable(inputs, outputs, name):\n",
    "    # why initialize weights this way?\n",
    "    # http://cs231n.github.io/neural-networks-2/\n",
    "    initial = tf.truncated_normal(shape=[inputs, outputs], stddev=1.0 / math.sqrt(float(inputs)))\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.0, shape=[shape])\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "weights1 = weight_variable(NUM_PIXELS, HIDDEN1_UNITS, \"weights1\")\n",
    "biases1 = bias_variable(HIDDEN1_UNITS, \"biases1\")\n",
    "hidden1 = tf.nn.relu(tf.matmul(x, weights1) + biases1, name=\"hidden1\")\n",
    "\n",
    "weights2 = weight_variable(HIDDEN1_UNITS, NUM_CLASSES, \"weights2\")\n",
    "biases2 = bias_variable(NUM_CLASSES, \"biases2\")\n",
    "\n",
    "y = tf.matmul(hidden1, weights2) + biases2\n",
    "\n",
    "# Write a summary of the graph (before we add the loss and optimizer)\n",
    "# Which will add a bunch of nodes automatically\n",
    "sw = tf.train.SummaryWriter('summaries/single_hidden', graph=tf.get_default_graph())\n",
    "sw.close()\n",
    "\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(TRAIN_STEPS):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "    _, loss = sess.run([train_step, cross_entropy], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "    if i % 200 == 0: print (\"loss %f\" % loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"Accuracy %f\" % sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                  y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Add a second hidden layer to the above code, with 64 units. Experiment with the parameters (batch size, steps, learning rate, units per layer) to see if you can achieve higher accuracy than the single hidden layer model. Keep in mind there's randomness between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your solution here or modify the above code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
